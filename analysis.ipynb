{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Read the CSV file\n",
    "titanic_data = pd.read_csv('dataset/titanic.csv')\n",
    "\n",
    "# Rename the data to a dataframe for semantics\n",
    "df = titanic_data\n",
    "\n",
    "# Let's see what the attribute values are:\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  \\\n",
      "0         0       3  22.0      1      0   7.2500      True       False   \n",
      "1         1       1  38.0      1      0  71.2833     False       False   \n",
      "2         1       3  26.0      0      0   7.9250     False       False   \n",
      "3         1       1  35.0      1      0  53.1000     False       False   \n",
      "4         0       3  35.0      0      0   8.0500      True       False   \n",
      "\n",
      "   Embarked_S  \n",
      "0        True  \n",
      "1       False  \n",
      "2        True  \n",
      "3        True  \n",
      "4        True  \n"
     ]
    }
   ],
   "source": [
    "# First thing we need to do is to clean the data to prepare it for PCA. Remember, PCA works on numerical data, \n",
    "# whilst most of the attributes below are qualitative.\n",
    "# Drop columns that are not useful for PCA or have too many missing values\n",
    "df_clean = df.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'])\n",
    "\n",
    "# Probably better ways to do this, but for now imma just drop the rows with missing values\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "# Converts a category attribute like sex and embarked into a binary attribute. This creates a column sex_male and drops sex_female as\n",
    "# it's redundant, either its true or not. Embarked has value S, C or Q so it creates embarked q and s, dropping embarked c as it's redundant,\n",
    "# done by drop_first=true.\n",
    "df_clean = pd.get_dummies(df_clean, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Let's see how the dataset looks like now:\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.82416338  0.90859974 -0.52766856 ...  0.75613751 -0.20232566\n",
      "   0.53403984]\n",
      " [ 1.21335165 -1.48298257  0.57709388 ... -1.32251077 -0.20232566\n",
      "  -1.87251946]\n",
      " [ 1.21335165  0.90859974 -0.25147795 ... -1.32251077 -0.20232566\n",
      "   0.53403984]\n",
      " ...\n",
      " [ 1.21335165 -1.48298257 -0.73481151 ... -1.32251077 -0.20232566\n",
      "   0.53403984]\n",
      " [ 1.21335165 -1.48298257 -0.25147795 ...  0.75613751 -0.20232566\n",
      "  -1.87251946]\n",
      " [-0.82416338  0.90859974  0.16280796 ...  0.75613751  4.94252683\n",
      "  -1.87251946]]\n"
     ]
    }
   ],
   "source": [
    "# The attributes above are of interest when it comes to the target variable being survivability and we can see that we have no more \n",
    "# missing attributes that are not numerical.\n",
    "\n",
    "# One last thing before we apply PCA, we need to standardize the data. This is because PCA is sensitive to the scale of the data.\n",
    "# One reason is that if we compare age to fare, age can vary from 0 to 100, whilst fare can vary from 0 to 1000. This means that\n",
    "# the variance in the data is dominated by fare. We need to standardize the data so that the variance in the data is not dominated\n",
    "# by one attribute. We can do this by subtracting the mean and dividing by the standard deviation of each attribute! \n",
    "\n",
    "# Standardize the data using the StandardScaler import, mathematically this is fairly simple, as it involves Z scoring the data\n",
    "# with Z = (X - myu) / sigma. This forms the basis of a normal distribution.\n",
    "scaler = StandardScaler()\n",
    "# fit calculates the mean and standard deviation of each attribute and transform applies the Z score formula to each attribute.\n",
    "df_standardized = scaler.fit_transform(df_clean)\n",
    "\n",
    "# Let's see how it looks like now:\n",
    "print(df_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first principal component explains 24.6113851183069 % of the variance.\n",
      "The second principal component explains 19.487247916098706 % of the variance.\n",
      "The third principal component explains 15.066051913553114 % of the variance.\n"
     ]
    }
   ],
   "source": [
    "# All the values above show how many standard deviations away from the mean the data is. Notice how there's one value that is 4.94\n",
    "# standard deviations away from the mean. This is an outlier that we should take into account when we do PCA.\n",
    "\n",
    "# Another thing to note is that by standardizing the data, we don't need to do\n",
    "# Y = df_standardized - np.mean(df_standardized, axis=0) anymore because standardizing already transforms the attribute so that the mean is 0.\n",
    "\n",
    "# Let's finally perform PCA via Single Value Decomposition svd:\n",
    "U, S, Vt = svd(df_standardized, full_matrices=False)\n",
    "\n",
    "# Compute variance explained by principal components\n",
    "# For example: if S is [3, 2, 1] then S * S is [9, 4, 1] and (S * S).sum() is 9 + 4 + 1 = 14.\n",
    "# rho is [9/14, 4/14, 1/14] = [0.64, 0.29, 0.07] where we get the variance explained by each principal component!\n",
    "rho = (S * S) / (S * S).sum()\n",
    "\n",
    "# Let's see how much variance is explained by the first 3 principal components:\n",
    "print(\"The first principal component explains\", rho[0] * 100, \"% of the variance.\")\n",
    "print(\"The second principal component explains\", rho[1] * 100, \"% of the variance.\")\n",
    "print(\"The third principal component explains\", rho[2] * 100, \"% of the variance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the first three principle components are ordered from highest to lowest variance explained.\n",
    "# Usually we want to define a threshold, which in our case will be 90% of the variance explained.\n",
    "\n",
    "threshold = 0.9\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
